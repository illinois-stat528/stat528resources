% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  ignorenonframetext,
]{beamer}
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{part title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{part title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\newif\ifbibliography
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{graphicx}
\usepackage{bm}
\usepackage{array}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{tikz-cd}
\usepackage{url}
\definecolor{foreground}{RGB}{255,255,255}
\definecolor{background}{RGB}{34,28,54}
\definecolor{title}{RGB}{105,165,255}
\definecolor{gray}{RGB}{175,175,175}
\definecolor{lightgray}{RGB}{225,225,225}
\definecolor{subtitle}{RGB}{232,234,255}
\definecolor{hilight}{RGB}{112,224,255}
\definecolor{vhilight}{RGB}{255,111,207}
\setbeamertemplate{footline}[page number]
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={STAT 528 - Advanced Regression Analysis II},
  pdfauthor={Exponential family theory},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}

\title{STAT 528 - Advanced Regression Analysis II}
\author{Exponential family theory}
\date{}
\institute{Daniel J. Eck\\
Department of Statistics\\
University of Illinois}

\begin{document}
\frame{\titlepage}

\begin{frame}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Proj}{\textbf{P}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\rootn}{\sqrt{n}}
\newcommand{\p}{\mathbf{p}}
\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}

\newtheorem{cor}{Corollary}
\newtheorem{lem}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{prop}{Proposition}
\end{frame}

\begin{frame}{Agenda for today}
\protect\hypertarget{agenda-for-today}{}
\begin{itemize}
\tightlist
\item
  Course software and GitHub
\item
  Go over basics of exponential family theory
\item
  Cover regular full exponential families and their properties
\item
  Discuss identifiability
\end{itemize}
\end{frame}

\begin{frame}{Exponential family}
\protect\hypertarget{exponential-family}{}
An \emph{exponential family of distributions} is a parametric
statistical model having log likelihood that takes the form
\begin{equation} \label{expolog}
    l(\theta) = \langle y,\theta \rangle - c(\theta),
\end{equation} where \(y\) is a vector statistic and \(\theta\) is a
vector parameter, and

\begin{itemize}
\tightlist
\item
  \(\langle y,\theta \rangle\) is the usual inner product,
\item
  \(c(\theta)\) is the cumulant function.
\end{itemize}

This uses the convention that terms that do not contain the parameter
vector can be dropped from a log likelihood; otherwise additional terms
also appear in \eqref{expolog}.

When the log likelihood can be expressed as \eqref{expolog} we say that
\(y\) is the \textbf{canonical statistic} and \(\theta\) is the
\textbf{canonical parameter}.
\end{frame}

\begin{frame}{Example: Binomial distribution}
\protect\hypertarget{example-binomial-distribution}{}
Let \(X \sim\) Binomial(\(n\),\(p\)) where \(0 < p < 1\). We can write
the log probability mass function for \(X\) \begin{align*}
  l(p) &= \log\left({n \choose x}\right) +  x\log(p) + (n-x)\log(1-p) \\
       &\propto  x\log(p) + (n-x)\log(1-p) 
\end{align*} in exponential family form \[
  l(\theta) = \langle y,\theta \rangle - c(\theta).
\]
\end{frame}

\begin{frame}{Densities}
\protect\hypertarget{densities}{}
Let \(w\) represent the full data, then the densities have the form
\begin{equation} \label{expodens}
  f_\theta(w) = h(w)\exp\left(\langle Y(w),\theta\rangle - c(\theta)\right)
\end{equation} and the word ``density'\,' here can refer to a PMF, PDF,
or to a density with respect to a positive measure.

The \(h(w)\) arises from any term not containing the parameter that is
dropped in going from log densities to log likelihood as we saw on the
previous slide.

The function \(h\) has to be nonnegative, and any point \(w\) such that
\(h(w) = 0\) is not in the support of any distribution in the family.
\end{frame}

\begin{frame}{Example: Binomial distribution}
\protect\hypertarget{example-binomial-distribution-1}{}
Let \(X \sim\) Binomial(\(n\),\(p\)) where \(0 < p < 1\). We can write
the probability mass function for \(X\) \[
  f_p(x) = {n \choose x}p^x(1-p)^{n-x}
\] as an exponential family density \[
 f_{\theta}(w) = h(w)\exp\left( \langle Y(w),\theta \rangle - c(\theta)\right).
\]
\end{frame}

\begin{frame}{Example: Normal distribution}
\protect\hypertarget{example-normal-distribution}{}
Let \(W \sim N(\mu, \sigma^2)\). Then we can write \[
    f_{\mu,\sigma^2}(w) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(w-\mu)^2}{2\sigma^2}\right) 
\] as an exponential family density \[
 f_{\theta}(w) = h(w)\exp\left( \langle Y(w),\theta \rangle - c(\theta)\right),
\] where \[
  c(\theta) = \frac{1}{2}\left(\frac{\theta_1^2}{2\theta_2} - \log(2\theta_2)\right).
\]
\end{frame}

\begin{frame}{Cumulant functions}
\protect\hypertarget{cumulant-functions}{}
Being a density, \eqref{expodens} must sum, integrate, or sum-integrate
to one. Hence, \begin{align*}
    1 &= \int f_\theta(w) dw \\ 
      &= \int \exp\left(\langle Y(w),\theta\rangle - c(\theta)\right) h(w)dw \\
      &= \exp\left(-c(\theta)\right) \int \exp\left(\langle Y(w),\theta\rangle\right) h(w) dw.
\end{align*} Rearranging the above implies that \[
  c(\theta) = \log\left(\int \exp\left(\langle Y(w),\theta\rangle\right) h(w) dw\right).
\]
\end{frame}

\begin{frame}{}
\protect\hypertarget{section}{}
The cumulant function is the log Laplace transformation corresponding to
the \emph{generating measure} given by \[
  \lambda(dw) = h(w)dw
\] when the random variable is continuous. Under this formulation \[
  c(\theta) = \log\left(\int \exp\left(\langle Y(w),\theta\rangle\right) \lambda(dw)\right).
\]

In our log likelihood based definition of the exponential family
\eqref{expolog}, the dropped terms which do not appear in the log
likelihood are incorporated into the counting measure (discrete
distributions) or Lebesgue measure (continuous distributions).
\end{frame}

\begin{frame}{Full families}
\protect\hypertarget{full-families}{}
Define \begin{equation} \label{parmspace}
  \Theta = \{ \theta : c(\theta) < \infty \}    .
\end{equation}

Then \eqref{expolog} and \eqref{expodens} define a distribution for all
\(\theta \in \Theta\).

We say an exponential family is \emph{full} if its canonical parameter
space is \eqref{parmspace}. Many commonly used statistical models are
full exponential families.
\end{frame}

\begin{frame}{Moment generating functions}
\protect\hypertarget{moment-generating-functions}{}
We no longer fuss about \(Y(w)\) and will suppress \(w\) when writing
\(Y\). We still mention the function \(h\) in \eqref{expodens} which is
now derived with respect to \(Y\) instead of \(w\).

The moment generating function of the canonical statistic, if it exists,
is given by \begin{equation} \label{mgf}
\begin{split}
    M_\theta(t) &= \text{E}_\theta\left(e^{\langle Y, t\rangle}\right) \\
      &= e^{c(\theta + t) - c(\theta)}.
\end{split}
\end{equation}

The moment generating function exists if \(\theta\) is an interior point
of the full canonical parameter space \eqref{parmspace}.
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-1}{}
By the theory of moment generating functions, if the moment generating
function exists, then moments of all orders exist and ordinary moments
are given by the derivatives of \(M_\theta(t)\) evaluated at zero.

In particular, \begin{align*}
  \text{E}_\theta(Y) &= \nabla M_\theta(0) = \nabla c(\theta) \\  
  \text{E}_\theta(YY^T) &= \nabla^2 M_\theta(0) = \nabla^2 c(\theta) + [\nabla c(\theta)][\nabla c(\theta)]^T.      
\end{align*}
\end{frame}

\begin{frame}{Cumulant generating function}
\protect\hypertarget{cumulant-generating-function}{}
A log moment generating function is called a
\emph{cumulant generating function} and its derivatives evaluated at
zero are called the \emph{cumulants} of the distribution.

For \(\theta\) in the interior of the full canonical parameter space
\(\Theta\), the cumulant generating function corresponding to the
canonical statistic is \begin{equation} \label{cgf}
  k_\theta(t) = c(t + \theta) - c(\theta),  
\end{equation} where \(c(\theta)\) is the cumulant function
corresponding to the exponential family in canonical form.
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-2}{}
The derivatives of \(k_\theta(t)\) evaluated at 0 are the same as the
cumulant function \(c\) evaluated at \(\theta\). The first and second
cumulants of the canonical statistic are \begin{equation} \label{cumrel}
\begin{split}
    \nabla c(\theta) &= \text{E}_\theta(Y) \\
    \nabla^2 c(\theta) &= \text{E}_\theta(YY^T) - \left[\text{E}_\theta(Y)\right]\left[\text{E}_\theta(Y)\right]^T = \text{Var}_\theta(Y).  
\end{split}
\end{equation}
\end{frame}

\begin{frame}{Regular exponential families}
\protect\hypertarget{regular-exponential-families}{}
An exponential family is regular if its full canonical parameter space
\eqref{parmspace} is an open set so that the moment and cumulant
generating functions exist for all \(\theta\).

The formulas in the preceding section hold for all \(\theta\).

Nearly every exponential family that arises in applications is regular.
We will not discuss non-regular exponential families.
\end{frame}

\begin{frame}{Example: Binomial distribution}
\protect\hypertarget{example-binomial-distribution-2}{}
The Binomial distribution with the standard parameter space
\(0 < p < 1\) written in canonical form is a regular full exponential
family.

We already saw that the loglikelihood for the Binomial distribution can
be written as \[
  l(\theta) = \langle Y,\theta \rangle - c(\theta).
\]

The canonical parameter \(\theta\) for this exponential family is
\(\theta = \log\left(\frac{p}{1-p}\right)\) and this implies that
\(c(\theta) < \infty\) for all \(\theta \in \mathbb{R}\) and that
\(\Theta\) is open.
\end{frame}

\begin{frame}{Take home message}
\protect\hypertarget{take-home-message}{}
Let \(Y\) be a regular full exponential family in canonical form. Then
\(Y\) has log likelihood given by \[
  l(\theta) = \langle Y,\theta \rangle - c(\theta),
\] and \begin{align*} 
    \text{E}_\theta(Y) &= \nabla c(\theta),  \\
    \text{Var}_\theta(Y) &= \nabla^2 c(\theta).   
\end{align*}
\end{frame}

\begin{frame}{Identifiability}
\protect\hypertarget{identifiability}{}
A statistical model is \emph{identifiable} if any two distinct parameter
values correspond to distinct distributions.

An exponential family fails to be identifiable if there are two distinct
canonical parameter values \(\theta\) and \(\psi\) such that the density
\eqref{expodens} of one with respect to the other is equal to one with
probability one.

This happens if \(Y^T(\theta - \psi)\) is equal to a constant with
probability one.

And this says that the canonical statistic \(Y\) is concentrated on a
hyperplane and the vector \(\theta - \psi\) is perpendicular to this
hyperplane.
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-3}{}
Conversely, if the canonical statistic \(Y\) is concentrated on a
hyperplane \begin{equation}\label{hyperplane}
  H = \{y : y^Tv = a\}  
\end{equation} for some non-zero vector \(v\), then for any scalar \(s\)
\[
  c(\theta + sv) = sa + c(\theta),
\] which immediately implies that \[
  l(\theta + sv) = l(\theta).
\]
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-4}{}
\begin{thm}
An exponential family fails to be identifiable if and only if the canonical statistic is concentrated on a hyperplane. If that hyperplane is given by \eqref{hyperplane} and the family is full, then $\theta$ and $\theta+sv$ are in the full canonical parameter space and correspond to the same distribution for every canonical parameter value $\theta$ and every scalar $s$.     
\end{thm}

\begin{itemize}
\tightlist
\item
  The direction \(sv\) along a vector \(v\) in the parameter space such
  that \(\theta\) and \(\theta + sv\) always correspond to the same
  distribution is called a \emph{direction of constancy}.
\item
  The theorem says that \(v\) is such a vector if and only if \(Y^Tv\)
  is constant with probability one.
\item
  It is clear from this that the set of all such vectors is closed under
  vector addition and scalar multiplication, hence is a vector subspace.
  This subspace is called the \emph{constancy space} of the family.
\end{itemize}
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-5}{}
\textbf{Note}: It is always possible to choose the canonical statistic
and parameter so the family is identifiable. \(Y\) being concentrated on
a hyperplane means some components are affine functions of other
components with probability one, and this relation can be used to
eliminate components of the canonical statistic vector until one gets to
an identifiable choice of canonical statistic and parameter. But this is
not always advisable. Prematurely enforcing identifiability may
complicate many theoretical issues.
\end{frame}

\begin{frame}{Example: Multinomial distribution}
\protect\hypertarget{example-multinomial-distribution}{}
We will show that a nonidentifiable parameterization allows for a
relatively routine argument to show that the multinomial distribution
can be written in canonical form.

First, some background on ratios of densities:

When we look at a ratio of two exponential family densities with
canonical parameter vectors \(\theta\) and \(\psi\), the \(h(w)\) term
cancels, and \begin{equation} \label{Radon}
  f_{\theta;\psi}(y) = e^{\langle y,\theta - \psi\rangle - c(\theta) + c(\psi)} 
\end{equation} is a density of the distribution with canonical parameter
\(\theta\) taken with respect to the distribution with canonical
parameter \(\psi\) (a Radon-Nikodym derivate in probability theory).
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-6}{}
In the same vein as \eqref{Radon}, we obtain the identity
\begin{equation} \label{cumident}
    c(\theta) = c(\psi) + \log\left(\mathrm{E}_{\psi}\left(e^{\langle Y, \theta - \psi \rangle}\right)\right)
\end{equation} Then \eqref{cumident} gives the following for the
multinomial distribution \begin{align*}
  c(\theta) &= c(\psi) + \log\left(\mathrm{E}_{\psi}\left(e^{\langle Y, \theta - \psi \rangle}\right)\right) \\
  &= c(\psi) + n\log\left(\sum_{i=1}^d p_ie^{\theta_i - \psi_i}\right),
\end{align*} where the last equality follows from the multinomial
theorem.
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-7}{}
Then \eqref{Radon} gives \begin{align*}
  f_{\theta}(y) &= f_{\psi}(y)e^{\langle y,\theta-\psi\rangle - c(\theta) + c(\psi)} \\
    &= {n \choose y} \left(\prod_{i=1}^d\left[p_ie^{\theta_i-\psi_i}\right]^{y_i}\right)\left(\sum_{i=1}^d p_ie^{\theta_i-\psi_i}\right)^{-n} \\
    &= {n \choose y} \prod_{i=1}^d \left(\frac{p_ie^{\theta_i-\psi_i}}{\sum_{j=1}^dp_je^{\theta_j-\psi_j}}\right)^{y_i}.
\end{align*}
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-8}{}
We simplify the above by choosing \(p\) and \(\psi\) so that
\(p_ie^{-\psi_i} = 1\) for all \(i\) and \(c(\psi) = 0\), so \[
  c(\theta) = n\log\left(\sum_{i=1}^d e^{\theta_i}\right)
\] and \[
  f_{\theta}(y) = {n \choose y}\prod_{i=1}^d \left(\frac{e^{\theta_i}}{\sum_{j=1}^d e^{\theta_j}}\right)^{y_i}
\] and this is the PMF of the multinomial distribution with sample size
\(n\) and probability vector having components \[
  p_i(\theta) = \frac{e^{\theta_i}}{\sum_{j=1}^d e^{\theta_j}}.
\]
\end{frame}

\end{document}
