---
title: "STAT 528 - Advanced Regression Analysis II"
author: "Binary response regression (part I)"
institute: |
  | Daniel J. Eck
  | Department of Statistics  
  | University of Illinois
date: ""
output: 
    beamer_presentation:
        keep_tex: true
        fig_width: 11
        fig_height: 7.5
        includes:
bibliography: ../note_sources.bib
urlcolor: blue
header-includes:
- \usepackage{graphicx}
- \usepackage{bm}
- \usepackage{array}
- \usepackage{amsmath}
- \usepackage{amsthm}
- \usepackage{amsfonts}
- \usepackage{amssymb}
- \usepackage{tikz-cd}
- \usepackage{url}
- \definecolor{foreground}{RGB}{255,255,255}
- \definecolor{background}{RGB}{34,28,54}
- \definecolor{title}{RGB}{105,165,255}
- \definecolor{gray}{RGB}{175,175,175}
- \definecolor{lightgray}{RGB}{225,225,225}
- \definecolor{subtitle}{RGB}{232,234,255}
- \definecolor{hilight}{RGB}{112,224,255}
- \definecolor{vhilight}{RGB}{255,111,207}
- \setbeamertemplate{footline}[page number]
---

\newcommand{\R}{\mathbb{R}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Proj}{\textbf{P}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\rootn}{\sqrt{n}}
\newcommand{\p}{\mathbf{p}}
\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\pibf}{\bm{\pi}}
\newcommand{\logit}{\text{logit}}


\newtheorem{cor}{Corollary}
\newtheorem{lem}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{prop}{Proposition}



```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE,tidy.opts=list(width.cutoff=40))
library(ggplot2)

mycols     = c("chartreuse3", "orangered", "deepskyblue3", "darkorchid1", "yellow")
dark_theme = theme(plot.background   = element_blank(), 
                   panel.background  = element_blank(),
                   #legend.background = element_blank(), legend.key = element_blank(),
                   axis.title.x      = element_text(size = 26, colour = "grey80",
                                                    margin=margin(10,0,0,0)),
                   axis.title.y      = element_text(size = 26, colour = "grey80",
                                                    margin=margin(0,20,0,0)),
                   axis.text         = element_text(size=18, color = "grey80"), 
                   text              = element_text(size=20),
                   axis.title        = element_text(size = 26),
                   legend.title      = element_text(size = 26, colour = "grey80"),
                   panel.border      = element_blank(),
                   panel.grid.major  = element_line(colour = "grey50"), 
                   panel.grid.minor  = element_line(colour = "grey30"))
```


## Last time 

- Optimization 
- Sufficiency 
- Maximum Entropy
- Wrap-up



## Learning Objectives Today

- logistic regression
- data analysis
- connecting theory to application



## Background 

We suppose that we have a sample of data $(y_i,x_i)$, $i = 1,\ldots, n$ where 

- $y_i$ is a scalar response variable 
- $x_i$ is a vector of predictors. 


Recall from the exponential family notes that the log likelihood of the exponential family is of the form
\begin{equation} \label{expolog}
	l(\theta) = \langle y, \theta \rangle - c(\theta),
\end{equation}
where 

- $y \in \R^n$ is a vector statistic having components 
- $\theta \in \R^n$ is the canonical parameter vector. 

In those notes $\theta$ is unconstrained and the likelihood \eqref{expolog} corresponds to a saturated regression model, one parameter for every observation.


## 

A canonical linear submodel of an exponential family is a submodel having parameterization
$$
  \theta = M\beta,
$$
and log likelihood
\begin{equation} \label{subloglike}
  l(\beta) = \langle M'y, \beta \rangle - c(M\beta).
\end{equation}

In an exponential family GLM, the saturated model canonical parameter vector $\theta$ is ``linked'' to the saturated model mean value parameter vector through the change-of-parameter mappings $g(\theta)$. 

\vspace{12pt}
We can write
$$
 \mu = \E_\theta(Y) = g(M\beta) 
$$
which implies that we can write
$$
  g^{-1}\left(\E_\theta(Y)\right) = M\beta.
$$


\vspace{12pt}
<!-- Therefore, a linear function of the canonical submodel parameter vector is linked to the mean of the exponential family through the inverse change-of-parameter mapping $g^{-1}$.  -->
This is the basis of exponential family generalized linear models with link function $g^{-1}$.




## Logistic regression model

\vspace{12pt}
The logistic regression model is one of the most widely used and studied GLMs in practice. 

\vspace{12pt}
It is [one of] the most important model for binary response data, being commonly used for a wide variety of applications. 

\vspace{12pt}
The logistic regression model is used for analyzing a binary response variable, $y_i \in \{0,1\}$  where a $1$ encodes a "success" and a 0 encodes a "failure." 

\vspace{12pt}
The logistic regression model allows for users to model the probability of success as a function of covariates. 


## 

For a binary response variable $Y$ and a vector of predictors $X$, let 
$$
  \pi(x) = P(Y = 1|X = x). 
$$  
The logistic regression model is then 
\begin{equation} \label{logit}
  \pi(x) = P(Y = 1|X = x) = \frac{\exp(x'\beta)}{1 + \exp(x'\beta)}.
\end{equation}
Equivalently, the \emph{logit} (log-odds) of the response variable has a linear relationship in the canonical submodel parameters:
$$
  \text{logit}(\pi(x)) = \log\left(\frac{\pi(x)}{1 - \pi(x)}\right) =  x^T\beta.
$$
In vector notation, we can express the above as 
$$
  \pibf = \frac{\exp(M\beta)}{1 + \exp(M\beta)} = \frac{1}{1 + \exp(-M\beta)} \quad \text{and} \quad 
    \text{logit}(\pibf) = M\beta
$$
where the above $\exp(\cdot)$ and logit$(\cdot)$ operations are understood as componentwise operations. 


## 

We will again connect the log likelihood of independent Bernoulli trials to canonical linear submodels 
\begin{align*}
  &\sum_{i=1}^n y_i\log(\pi(x_i)) + (1-y_i)\log(1 - \pi(x_i)) \\
    &\qquad= \sum_{i=1}^n y_i\log\left(\frac{\pi(x_i)}{1 - \pi(x_i)}\right) - \log(1 - \pi(x_i)) \\
    &\qquad= \sum_{i=1}^n y_ix_i'\beta - \log(1 + \exp(x_i'\beta)) \\
    &\qquad= \langle M'y, \beta \rangle - c_\beta(\beta), 
\end{align*}
where $M$ has rows $x_i'$.


## Takeaways

- The logistic regression model is an exponential family model whose log likelihood can be written in canonical form. As such, the nice properties discussed over the last four lectures hold for this model.
- Note the differences between logistic and linear regression: the logistic regression model does not possess an additive error structure (ie signal plus noise); the change of parameters map $g$ is not the identity function; the mean-value parameter is a success probability
- In the first point above, it is interesting to note how [John A Nelder](https://en.wikipedia.org/wiki/John\_Nelder), one of the creators of GLMs, identifies the statistics of his day as too focused on mathematical properties of error distributions instead of studying the mechanisms of signal which is of interest to scientists and technologists (See Section 2 of [this paper](https://rss.onlinelibrary.wiley.com/doi/abs/10.2307/2981525)).



## Data Analysis 

- We will now apply logistic regression on a data set
- We will teach some basic data wrangling steps using \texttt{dplyr} within \texttt{tidyverse}



## The \texttt{dplyr} package within \texttt{tidyverse}

```{r, message=FALSE}
#install.packages("tidyverse")
library(tidyverse)
```

\texttt{dplyr} provides comprehensive tools for data manipulations (or wrangling). The five main "verbs" are: 

  - \texttt{select()}: choose from a subset of the columns
  
  - \texttt{filter()}: choose a subset of the rows based on logical criteria
  
  - \texttt{arrange()}: sort the rows based on values of the columns.
  
  - \texttt{mutate()}: add or modify the definitions of the column, and create columns that are functions of existing columns.
  
  - \texttt{summarise()}: collapse a data frame down to a single row (per group) by aggregating vectors into a single value. Often used in conjunction with \texttt{group\_by()}



## The pipe operator 

The pipe operator \texttt{\%>\%} allows for verbs to be strung in succession so that complicated manipulations can be combined within a single easily digestible sentence.

\vspace{12pt}
```{r, eval=FALSE}
data %>% 
	inner_function() %>% 
	outer_function()
```



## Example: CCSO data

We will demonstrate logistic regression modeling on the [Champaign County Sheriff's Office (CCSO) data frame](https://github.com/CUHackNight/JailData). 

\vspace{12pt}
We now load in the CCSO data frame using the fread (**f**ast **read**) function from the \texttt{data.table} package (can also use read\_csv in the \texttt{tidyverse}) and perform most of our data wrangling operations using \texttt{dplyr}.

\vspace{12pt}
```{r, warnings = FALSE, message = FALSE}
rm(list = ls())
library(tidyverse)
library(data.table)
```



##

Let's load in and wrangle the data.

\tiny
```{r loaddata, cache = TRUE}
## load in data
system.time(CCSO <- fread("https://uofi.box.com/shared/static/9elozjsg99bgcb7gb546wlfr3r2gc9b7.csv"))
dim(CCSO)

## data wrangling
CCSO_small <- CCSO %>% rename(Days = "Days in Jail", Age = "Age at Arrest", 
                        Date = "BOOKING DATE", Sex = "SEX", Race = "RACE",
                        Crime = "CRIME CODE", Agency = "ARREST AGENCY") %>% 
  mutate(atleastone = ifelse(Days > 0,1,0)) %>% 
  filter(Crime == "OTHER TRAFFIC OFFENSES") %>%  
  filter(Race %in% c("Asian/Pacific Islander","Black","White","Hispanic")) %>% 
  filter(Sex %in% c("Female","Male")) %>% 
  dplyr::select(atleastone, Age, Sex, Date, Race) %>%  
  mutate(Race = fct_drop(Race), Sex = fct_drop(Sex))
CCSO_small <- CCSO_small[complete.cases(CCSO_small), ]

head(CCSO_small, 5)
dim(CCSO_small)
```



## 

In this analysis we will investigate the propensity of incarcerations lasting longer than one day for crimes encoded as "other traffic offenses" where: 

- The response variable is `atleastone` where a 1 indicates an incarceration lasting longer than one day and a 0 indicates an incarceration lasting shorter than 1 day. 
- The covariates are: Age (age at arrest), Sex (Male or Female), Race (Asian/Pacific Islander, Black, Hispanic, and White). 

\vspace{12pt}
Note: this data set is observational. We did have any control over who entered the data set or how long incarcerations were or anything else. **Why is this important to note?**


## 

We can fit a basic main effects model in an instant using the glm function in R.

\vspace{12pt}
\tiny
```{r}
m1 <- glm(atleastone ~ -1 + Race + Sex + Age, data = CCSO_small, 
          family = "binomial", x = "TRUE")
```


\vspace{12pt}
\normalsize
Now let's unpack the glm function call above. We decided that we wanted to fit an exponential family regression model with log likelihood taking the general form
$$
  l(\beta) = \langle M'y, \beta \rangle - c(M\beta),
$$
where $M$ specified by the formula in the glm function call above. 

\tiny
```{r}
M <- m1$x
head(M)
```



## 

The specific log likelihood for the logistic regression model can then be written as
$$
  l(\beta) \propto \sum_{i=1}^n y_ix_i^T\beta - \log\left(1 + \exp(x_i^T\beta)\right),
$$

\vspace{12pt}
The glm function then performs a Fisher scoring based optimization routine (technically IRLS) to maximize the above likelihood. It stores $\hat\beta$ among many other useful quantities.

\vspace{12pt}
\tiny
```{r}
names(m1)
```



## 

We can view summary information for $\hat\beta$ and the fitting process using the summary function

\tiny
```{r}
summary(m1)
```



## 

Recall from the asymptotic theory of maximum likelihood estimation that 
$$
  \sqrt{n}(\hat\beta - \beta) \overset{d}{\to} N(0, \Sigma^{-1}),
$$
where $\Sigma^{-1}$ is the inverse of the Fisher information matrix. We can extract these same standard errors using the vcov function

\vspace{12pt}
\tiny
```{r}
sqrt(diag(vcov(m1)))
```

\vspace{12pt}
\normalsize
These values are the same as those in the Std. Error column in the above summary table

\vspace{12pt}
\tiny
```{r}
all.equal(summary(m1)$coef[, 2], sqrt(diag(vcov(m1))))
```



## Inference

Recall that we can make inferences about $\beta_j$ using the Wald statistic corresponding to the hypothesis test
$$
  H_o: \beta_j = 0, \qquad H_a:\beta_j \neq 0,
$$
which is given by
$$
  \frac{\hat{\beta_j}}{\text{se}(\hat\beta_j)} \sim N(0,1),
$$


We can compute the p-values by hand

\tiny
```{r}
Z <- abs(coef(m1)/sqrt(diag(vcov(m1))))
round(2*pnorm(Z, lower = FALSE), 4)
```



## Deviance and likelihood ratio testing 

Recall our $l(\mu;y)$ notation.

\vspace{12pt}
The deviance is defined by
$$
 -2\left[l(\hat\mu;y) - l(y;y)\right].
$$
This is the likelihood-ratio for testing the null hypothesis that the model against the general alternative (ie, the saturated model). The deviance has reference distribution
$$
  -2\left[l(\hat\mu;y) - l(y;y)\right] \approx \chi^2_{\text{df}}
$$
where df = $n - p$, $n$ is the sample size, and $p$ is the number of model parameters.


## 

We do in fact obtain sufficient dimension reduction.

```{r}
## compare with saturated model
m1$deviance
m1$df.residual
pchisq(m1$deviance, df = m1$df.residual, lower = FALSE)
```



##

Can compute test by hand. First,
$$
l(y;y) = \sum_{i=1}^n y_i\log(y_i) + (1 - y_i)\log(1 - y_i) = 0.
$$
And $l(y; \hat{\mu})$ equals
```{r}
logLik(m1)
```

\vspace{12pt}
The deviance is the same
\vspace{12pt}
\tiny
```{r}
n <- nrow(CCSO_small); p <- length(coef(m1))
-2 * logLik(m1) == m1$deviance
n - p == m1$df.residual
as.numeric(pchisq(-2 * logLik(m1), df = n - p, lower = FALSE))
```



## Compare with null model 

Fit the null model and perform test in R.

\vspace{12pt}
\tiny
```{r}
## use LRT testing in the anova function 
m_null <- glm(atleastone ~ 1, family = "binomial", data = CCSO_small)
anova(m_null, m1, test = "LRT")
```


\vspace{12pt}
\normalsize
Fit the model with $\hat{\mu} = \bar{y}$ and compute test by hand.
\tiny
```{r}
y <- CCSO_small$atleastone
prob <- mean(y)
round(-2 * sum(y*log(prob) + (1-y)*log(1-prob)), 3) == round(m_null$deviance, 3)
Xstat <- -2 * sum(y*log(prob) + (1-y)*log(1-prob)) + 2 * logLik(m1)
pchisq(Xstat, df = p - 1, lower = FALSE)
```


## A point about model matrices

Recall that span($M$) is important in the sense that 
\begin{align*}
\langle y, M_1\beta_1 \rangle - c(M_1\beta_1) = \langle y, M_2\beta_2 \rangle - c(M_2\beta_2)
\end{align*}
when span($M_1$) = span($M_2$).

\vspace{12pt}
\tiny
```{r}
m1_2 <- glm(atleastone ~ Race + Sex + Age, data = CCSO_small, 
          family = "binomial", x = "TRUE")
round(logLik(m1), 4) == round(logLik(m1_2), 4)

coef(m1)
coef(m1_2)
```



## 

Now let's consider an interesting competing model 

\tiny
```{r}
CCSO_small <- CCSO_small %>% mutate(isBlack = ifelse(Race == "Black",1,0))
m2 <- glm(atleastone ~ isBlack + Sex + Age, data = CCSO_small, 
          family = "binomial", x = "TRUE")
summary(m2)
```



## 

Is this new model nested within our old model? In other words, will 
```{r, eval = FALSE}
anova(m2, m1, test == "LRT")
```
work?

\vspace{12pt}
\tiny
```{r}
head(m2$x, 3)
head(m1$x, 3)
M <- cbind(rowSums(m1$x[, c(1,3,4)]), m1$x[, -c(1,3,4)])
head(M, 3)
logLik(glm(y ~ M, family = "binomial"))
logLik(m2)
```


##

\small
```{r}
anova(m2, m1, test = "LRT")
```


\vspace{12pt}
What does this test tell us?



## Other parameterizations

Recall: 

![](transformations.png)


## 

We start with 
$$
  \langle M'y, \beta \rangle - c_\beta(\beta),
$$
and then obtain $\hat{\beta}$ by maximizing the above. From here: 

- $\hat{\theta} = M\hat{\beta}$
- $\hat\mu = \nabla_\theta c(\theta^*)|_{\theta* = \hat{\theta}}$
- $\hat\tau = M'\hat{u}$

\vspace{12pt}
For example we can compute $\hat\theta$ using the predict function or by hand.
\vspace{12pt}
\tiny
```{r}
theta <- m1$x %*% coef(m1)
head(cbind(predict(m1, type = "link"), theta), 3)
```


## 

The submodel canonical parameterization scale is bit awkward for interpretation, although summary tables provide some insight to which components of the submodel canonical parameter vector may be driving the data generating process (under the assumed model). 

\vspace{12pt}
R software provides functionality for estimating the mean value parameters 
$$
  \E(Y|X = x) = \Prob(Y = 1| X = x),
$$  
associated with every individual in the study. 

\vspace{12pt}
\tiny
```{r}
p1 <- predict(m1, type = "response", se.fit = TRUE)

## compute by hand
mu <- 1/(1 + exp(-theta))
head(cbind(p1$fit, mu), 3)
```



## Another point about model matrices 

Recall that model \texttt{m1\_2} did not suppress the intercept. 

\vspace{12pt}
This model's model matrix has the same span as the model that we have been working with. Mean-value parameters are the same for both models.


\vspace{12pt}
\small
```{r}
p1_2 <- predict(m1_2, type = "response", se.fit = FALSE)
head(cbind(p1$fit, p1_2), 6)
```


