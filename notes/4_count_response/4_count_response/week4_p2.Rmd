---
title: "STAT 528 - Advanced Regression Analysis II"
author: "Count response regression (part I)"
institute: |
  | Daniel J. Eck
  | Department of Statistics  
  | University of Illinois
date: ""
output: 
    beamer_presentation:
        keep_tex: true
        fig_width: 11
        fig_height: 7.5
        includes:
bibliography: ../note_sources.bib
urlcolor: blue
header-includes:
- \usepackage{graphicx}
- \usepackage{bm}
- \usepackage{array}
- \usepackage{amsmath}
- \usepackage{amsthm}
- \usepackage{amsfonts}
- \usepackage{amssymb}
- \usepackage{tikz-cd}
- \usepackage{url}
- \definecolor{foreground}{RGB}{255,255,255}
- \definecolor{background}{RGB}{34,28,54}
- \definecolor{title}{RGB}{105,165,255}
- \definecolor{gray}{RGB}{175,175,175}
- \definecolor{lightgray}{RGB}{225,225,225}
- \definecolor{subtitle}{RGB}{232,234,255}
- \definecolor{hilight}{RGB}{112,224,255}
- \definecolor{vhilight}{RGB}{255,111,207}
- \setbeamertemplate{footline}[page number]
---

\newcommand{\R}{\mathbb{R}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Proj}{\textbf{P}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\rootn}{\sqrt{n}}
\newcommand{\p}{\mathbf{p}}
\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\mubf}{\bm{\mu}}
\newcommand{\logit}{\text{logit}}

\newtheorem{cor}{Corollary}
\newtheorem{lem}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{prop}{Proposition}



```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE,tidy.opts=list(width.cutoff=40))
library(ggplot2)

mycols     = c("chartreuse3", "orangered", "deepskyblue3", "darkorchid1", "yellow")
dark_theme = theme(plot.background   = element_blank(), 
                   panel.background  = element_blank(),
                   #legend.background = element_blank(), legend.key = element_blank(),
                   axis.title.x      = element_text(size = 26, colour = "grey80",
                                                    margin=margin(10,0,0,0)),
                   axis.title.y      = element_text(size = 26, colour = "grey80",
                                                    margin=margin(0,20,0,0)),
                   axis.text         = element_text(size=18, color = "grey80"), 
                   text              = element_text(size=20),
                   axis.title        = element_text(size = 26),
                   legend.title      = element_text(size = 26, colour = "grey80"),
                   panel.border      = element_blank(),
                   panel.grid.major  = element_line(colour = "grey50"), 
                   panel.grid.minor  = element_line(colour = "grey30"))
```


## Last time 

- basic diagnostics for binary response models
- probit regression and threshold modeling
- basic causal inference



## Learning Objectives Today

- Poisson regression
- Residual diagnostics 
- Data analysis


## Background (again)

We suppose that we have a sample of data $(y_i,x_i)$, $i = 1,\ldots, n$ where 

- $y_i$ is a scalar response variable 
- $x_i$ is a vector of predictors. 


Recall from the exponential family notes that the log likelihood of the exponential family is of the form
\begin{equation} \label{expolog}
	l(\theta) = \langle y, \theta \rangle - c(\theta),
\end{equation}
where 

- $y \in \R^n$ is a vector statistic having components 
- $\theta \in \R^n$ is the canonical parameter vector. 

In those notes $\theta$ is unconstrained and the likelihood \eqref{expolog} corresponds to a saturated regression model, one parameter for every observation.


## 

A canonical linear submodel of an exponential family is a submodel having parameterization
$$
  \theta = M\beta,
$$
and log likelihood
\begin{equation} \label{subloglike}
  l(\beta) = \langle M'y, \beta \rangle - c(M\beta).
\end{equation}

In an exponential family GLM, the saturated model canonical parameter vector $\theta$ is ``linked'' to the saturated model mean value parameter vector through the change-of-parameter mappings $g(\theta)$. 

\vspace{12pt}
We can write
$$
 \mu = \E_\theta(Y) = g(M\beta) 
$$
which implies that we can write
$$
  g^{-1}\left(\E_\theta(Y)\right) = M\beta.
$$


## Poisson regression model 

The Poisson regression model [and its variants] is one of the more widely used and studied exponential family GLMs in practice. 

\vspace{12pt}
The Poisson regression model is used for analyzing a count response variable, $y_i \in \{0,1,2,\ldots\}$.

\vspace{12pt}
The Poisson regression model allows for users to model expected counts as a function of covariates. 


## preliminaries 

Recall the Poisson distributions with mass function
$$
  P(Y = y) = \frac{\mu^ye^{-\mu}}{y!}, \qquad (y = 0,1,2,\ldots),
$$
where $E(Y) = \mu$ and Var$(Y) = \mu$.



## 

For a count response variable $Y$ and a vector of predictors $X$, let $\mu(x) = \E(Y|X = x)$. The Poisson regression model is then 
\begin{equation} \label{loglink}
  \mu(x) = \E(Y|X = x) = \exp\left(x'\beta\right).
\end{equation}
Equivalently, 
$$
  \log\left(\mu(x)\right) = x'\beta.
$$
In vector notation, we can express the above as 
$$
  \mubf = \exp(M\beta) \quad \text{and} \quad \text{log}(\mubf) = M\beta
$$
where the above $\exp(\cdot)$ and log$(\cdot)$ operations are understood as componentwise operations.



## 

Let's consider the log likelihood of a sample of independent Poisson random variables 
\begin{align*}
  \sum_{i=1}^n y_i\log(\mu_i) - \mu_i 
    &=  \sum_{i=1}^n y_i\theta_i - \exp(\theta_i)
\end{align*}
where 
$$
  \theta_i = \log\left(\mu_i\right) = g^{-1}(\mu_i) \qquad \text{and} \qquad \mu_i = \exp(\theta_i) = g(\theta_i).
$$


We see that the Poisson regression model with log link is a canonical linear submodel of an exponential family when we write
$$
  \theta_i = x_i'\beta.
$$



## Example: Gala data

We will demonstrate Poisson regression modeling on the Galapagos data frame in the \texttt{faraway} package. This data frame consists of $n = 30$ observations and $7$ variables in total.

\vspace{12pt}
For 30 Galapagos Islands, we have: 

- a count of the number of plant species found on each island 
- five geographic variables for each island

\vspace{12pt}
A few missing values have been filled in for simplicity. We will model the number of species using Poisson regression using the glm function in R. 


## Galapagos variables 

- **Species**: the number of plant species found on the island
- **Area**: the area of the island (km$^2$)
- **Elevation**: the highest elevation of the island (m)
- **Nearest**: the distance from the nearest island (km)
- **Scruz**: the distance from Santa Cruz island (km)
- **Adjacent**: the area of the adjacent island (square km)

\vspace{12pt}
M. P. Johnson and P. H. Raven (1973) "[Species number and endemism: The Galapagos Archipelago revisited](https://pubmed.ncbi.nlm.nih.gov/17832770/)" Science, 179, 893-895


## 

We first load in necessary software.

\vspace{12pt}
\small
```{r, warnings = FALSE, message = FALSE}
rm(list = ls())
library(tidyverse)
library(faraway)
```

\vspace{12pt}
We create a discrete size variable based on the Area variable for demonstration purposes, and then fit the Poisson regression model

\vspace{12pt}
\footnotesize
```{r}
gala <- gala %>% 
  mutate(Size = as.factor(1 + ifelse(Area > 1,1,0) + 
                            ifelse(Area > 25,1,0)))
m1 <- glm(Species ~ Elevation + Nearest + Scruz + Adjacent + Size, 
          family = "poisson", data = gala, x = TRUE)
```



## 

As in logistic regression, we are going to unpack the glm call.


\vspace{12pt}
The specific log likelihood for the Poisson regression model is
$$
  l(\beta) \propto \sum_{i=1}^n y_ix_i'\beta - \exp\left(x_i'\beta\right)
$$
where 

- $x_i'$s are the rows of the design matrix $M$ 
- $y_i$s are the components of the response vector $y$ (the Species variable corresponding to the number of species on each of the islands) 
- $\beta$ is the submodel canonical parameter vector

\vspace{12pt}
The glm function then performs a Fisher scoring based optimization routine to maximize the above likelihood.


## 

We can view summary information for $\hat\beta$ and the fitting process using the summary function

\tiny
```{r}
summary(m1)
```


## 

Keep in mind that interpretations of $\beta$ are on the log scale. 

\vspace{12pt}
A particular component of $\hat{\beta}$ gives the estimated change in logs of expected counts when one makes a unit change to a particular component of the covariate vector with all other components held fixed.

\vspace{12pt}
Similar story for logistic regression. A particular component of $\hat{\beta}$ gives the estimated change in log odds when one makes a unit change to a particular component of the covariate vector with all other components held fixed.

\vspace{12pt}
This is good to keep in mind and it *may* be instructive for how you answer homework questions. But keep in mind that:  

- $\beta$ is given by $M$ and not span$(M)$
- modeling of expectations is the motivation for these models


## Demonstration of this point

\tiny
```{r}
m2 <-  glm(Species ~ -1 + Elevation + Nearest + Scruz + Adjacent + Size, 
          family = "poisson", data = gala, x = TRUE)
summary(m2)
```


## 

We see that

\vspace{12pt}
```{r}
logLik(m1) - logLik(m2)
```

\vspace{12pt}
\normalsize
and 

\vspace{12pt}
\footnotesize
```{r}
theta1 <- m1$x %*% coef(m1)
theta2 <- m2$x %*% coef(m2)
all.equal(theta1, theta2)
```

##

However, $\hat{\beta}_1 \neq \hat{\beta}_2$

\vspace{12pt}
\tiny
```{r}
coef(m1)
coef(m2)
```



## Other parameterizations

Recall: 

![](transformations.png)

## 

We start with 
$$
  \langle M'y, \beta \rangle - c_\beta(\beta),
$$
and then obtain $\hat{\beta}$ by maximizing the above. From here: 

- $\hat{\theta} = M\hat{\beta}$
- $\hat\mu = \nabla_\theta c(\theta^*)|_{\theta* = \hat{\theta}}$
- $\hat\tau = M'\hat{u}$

\vspace{12pt}
For example we can compute $\hat\theta$ using the predict function or by hand.
\vspace{12pt}
\tiny
```{r}
theta <- m1$x %*% coef(m1)
head(cbind(predict(m1, type = "link"), theta), 3)
```



## Inference 

The standard error column of the summary table are estimates of the square root of the variances of the estimated submodel canonical statistic vector $\hat\beta$. 

\vspace{12pt}
Recall from the asymptotic theory of MLE that 
$$
  \sqrt{n}(\hat\beta - \beta) \overset{d}{\to} N(0, \Sigma^{-1}),
$$
where $\Sigma^{-1}$ is the inverse of the Fisher information matrix. 


## 
We can extract these same standard errors using the vcov function

\vspace{12pt}
\tiny
```{r}
sqrt(diag(vcov(m1)))
```

\vspace{12pt}
\normalsize
These values are the same as those in the Std. Error column in the above summary table

\vspace{12pt}
\tiny
```{r}
all.equal(summary(m1)$coef[, 2], sqrt(diag(vcov(m1))))
```


## 

We can make inferences about $\beta_j$ using the Wald statistic corresponding to the hypothesis test
$$
  H_o: \beta_j = 0, \qquad H_a:\beta_j \neq 0,
$$
which is given by
$$
  \frac{\hat{\beta_j}}{\text{se}(\hat\beta_j)} \sim N(0,1),
$$
where this distributional relationship holds under the null hypothesis $\beta_j = 0$. Similarly, we can form a confidence interval
$$
  \hat\beta_j \pm z_{\alpha/2}\text{se}(\hat\beta_j)
$$
where $0 < \alpha < 1$ is some error threshold.


## Deviance and likelihood ratio testing 

Recall our $l(\mu;y)$ notation.

\vspace{12pt}
The deviance is defined by
$$
 -2\left[l(\hat\mu;y) - l(y;y)\right].
$$
This is the likelihood-ratio for testing the null hypothesis that the model against the general alternative (ie, the saturated model). The deviance has reference distribution
$$
  -2\left[l(\hat\mu;y) - l(y;y)\right] \approx \chi^2_{\text{df}}
$$
where df = $n - p$, $n$ is the sample size, and $p$ is the number of model parameters.


## 

We can also test nested models using deviance based testing
$$
  -2\left[l(\hat\mu_1;y) - l(\hat\mu_2;y)\right] \approx \chi^2_{\text{df}}
$$
where 

- $\hat{\mu_1}$ corresponds to a smaller model than that which led to estimation of $\hat{\mu}_2$
- df = $p_2 - p_1$


## 

Let's consider the smaller model that ignores the Elevation variable. A likelihood ratio test shows that the larger model is preferable at any reasonably chosen significance level $\alpha$.

\vspace{12pt}
\tiny
```{r}
m_small <- glm(Species ~ Nearest + Scruz + Adjacent + Size, 
          family = "poisson", data = gala, x = TRUE)

## built in likelihood ratio test using anova.glm
anova(m_small, m1, test = "LRT")

## perform the above directly (different machine tolerances)
pchisq(m_small$deviance - m1$deviance, df = 1, lower = FALSE)
```


## 

In this example we see that the levels of the Size variable are statistically significant at any reasonable error threshold $\alpha$. 

\vspace{12pt}
Suppose instead that we wanted to test if large islands are expected to have a different number of species than medium sized islands. 

\vspace{12pt}
Informally, the summary table above suggests that large islands have mores species than medium sized islands, but this is not a formal comparison. 

\vspace{12pt}
Formally, we want to test
$$
  H_o: \mu_l - \mu_m = 0, \qquad H_a: \mu_l - \mu_m \neq 0,
$$
where $\mu_l$ and $\mu_m$, respectively, correspond to the mean-value parameters for large and medium-sized islands.


##

We know that $\mu_l = \exp(\beta_l)$ and $\mu_m = \exp(\beta_m)$ where $\beta_l$ and $\beta_m$, respectively, correspond to the canonical parameters for large and medium-sized islands. 

\vspace{12pt}
We can test the hypothesis on the previous slide using the Delta method
$$
  \sqrt{n}\left[ (\hat\mu_l - \hat\mu_m) - (\mu_l - \mu_m)  \right] 
    \overset{d}{\to} N\left(0, \nabla h(\beta)'\Sigma^{-1}\nabla h(\beta)\right)
$$
where 
$$
  h(\beta) = \exp(\beta_l) - \exp(\beta_m) = \mu_l - \mu_m.
$$

## 

We obtain the estimate $\hat\mu_l - \hat\mu_m$ below

\vspace{12pt}
\tiny
```{r}
comp <- c(0,0,0,0,0,-1,1)
betahat <- m1$coefficients
grad <- exp(betahat) * comp
est <- sum(grad)
est
```

\vspace{12pt}
\normalsize
and the estimate of $\nabla h(\beta)$ is the column vector below

\vspace{12pt}
\tiny
```{r}
grad
```


##

The asymptotic variance $\nabla h(\beta)^T\Sigma^{-1}\nabla h(\beta)$ and corresponding standard error are estimated below

\vspace{12pt}
\tiny
```{r}
InvFish <- vcov(m1)
asympVar <- as.numeric(t(grad) %*% InvFish %*% grad)
asympVar
SE <- sqrt(asympVar)
SE
```


\vspace{12pt}
\normalsize
The ratio $(\hat\mu_l - \hat\mu_m)/\text{se}(\hat\mu_l - \hat\mu_m)$ is given by

\vspace{12pt}
\tiny
```{r}
est/SE
```


\vspace{12pt}
\normalsize
and a corresponding 95\% confidence interval is

\vspace{12pt}
\tiny
```{r}
est + qnorm(c(0.025,0.975)) * SE
```



## Note

Keep in mind that there are three possible tests that we could have made. We can adjust for this using a Bonferroni correction


\vspace{12pt}

```{r}
est + qnorm(c(0.025/3, 1-0.025/3)) * SE
```


## Residual diagnostics

Residuals represent the discrepancy between the model and the observed data, and are essential for exploring the adequacy of the model. 

\vspace{12pt}
In the Gaussian case, the residuals are $\hat\epsilon = y - \hat\mu$. In Faraway, these are referred to as response residuals for GLMs and they can be used directly to check the constant variance assumption in linear models with Gaussian errors. 

\vspace{12pt}
However, since the variance of the GLM is often not constant and is often a function of the canonical parameter, some modifications to the residuals are necessary.


## Pearson residuals

The Pearson residual is comparable to the standardized residual used for linear models and is defined as: 
$$
  r_P = \frac{y - \hat\mu}{\sqrt{\mathrm{Var}(\hat\mu)}}
$$
where 
 - $\mathrm{Var} = \nabla^2 c(\theta)$ is the estimated variance under the original exponential family. 
 
\vspace{12pt}
Notice that $\sum_{i=1}^n r_{P,i}^2$ is the Pearson $\chi^2$ statistic, hence the name. Pearson residuals can be skewed for nonnormal responses. 


## Deviance residuals

The deviance residuals are defined by analogy to the Pearson residuals. In other words, we set the deviance residual $r_D$ so that 
$$
  \sum_{i=1}^n r_{D,i}^2 = \text{Deviance} = \sum_{i=1}^n d_i,
$$
and 
$$
  r_{D,i} = \text{sign}(y_i - \hat\mu_i)\sqrt{d_i}.
$$
In Poisson regression the deviance residuals are
$$
  r_{D,i} = \text{sign}(y_i - \hat\mu_i)\left[2\left(\frac{y_i\log(y_i)}{\hat\mu_i} - y_i + \hat\mu_i\right)\right]^{1/2}.
$$



## 

We now revisit the Galapagos data to explore these residuals.

\vspace{12pt}
\tiny
```{r}
## Deviance residuals
head(residuals(m1))

## Pearson residuals
head(residuals(m1, "pearson"))
```


## 

\small
\vspace{12pt}
For GLMs, we must decide on the appropriate scale for the fitted values. Usually, it is better to plot on the scale of the linear predictors ($\theta$) than on the fitted responses ($\mu$). 

\vspace{12pt}
We are looking for features in these residuals vs. fitted values plots: 

- First of all, is there any nonlinear relationship between the residuals and the fitted values? If so, this would be an indication of a lack of fit that might be rectified by a change in the model. For a linear model, we may transform the response variable but this is likely impractical for a GLM since it would change the assumed distribution of the response variable. 
- We might also consider changing the link function, but often this undesirable since the canonical link functions facilitate desirable theoretical properties and yield models which are relatively easy to interpret. 

\vspace{12pt}
It is best to make a change in the choice of predictors or transformations to these predictors since this involves the least disruption to the GLM theoretical foundations.


## 

The plots below show the residuals as a function of $\hat\theta_i = x_i^T\hat\beta$.

\vspace{12pt}
\tiny
```{r}
theta <- as.numeric(m1$x %*% coef(m1))
par(mfrow = c(1,2))
plot(theta, residuals(m1), xlab = "theta", ylab = "Deviance residuals", pch = 19)
plot(theta, residuals(m1, "pearson"), pch = 19,  
     xlab = "theta", ylab = "Pearson residuals")
```


##

\small
The relationship between the mean and the variance is shown below. A line showing that the variance increases linearly in the mean (not a perfect slope of 1) is also shown.

\vspace{12pt}
\tiny
```{r}
plot(log(fitted(m1)),log((gala$Species-fitted(m1))^2), 
     xlab= expression(hat(mu)),ylab=expression((y-hat(mu))^2), pch = 19)
abline(0,1)
```


##

We see that the variance is proportional to, but larger than, the mean. 

\vspace{12pt}
When the variance assumption of the Poisson regression model is broken but the link function and choice of predictors are correct, the estimates of $\beta$ are consistent, but the standard errors will be wrong.

\vspace{12pt}
This problem is called overdispersion.



## (further reading)

We note that a deviance based test tells us that our submodel does not fit the data better than the full saturated model


\tiny
```{r}
## compare with saturated model
m1$deviance
m1$df.residual
pchisq(m1$deviance, df = m1$df.residual, lower = FALSE)
```



## 

The model misses bad in some spots

\vspace{12pt}
\tiny
```{r}
foo <- data.frame(names = rownames(gala),
                  obs = gala$Species,
                  pred = as.numeric(predict(m1, type = "response")), 
                  resid = residuals(m1))
head(cbind(foo %>% arrange(desc(obs)) %>% pull(names),
           foo %>% arrange(desc(pred)) %>% pull(names)), 10)
```


## 



```{r}
foo %>% arrange(desc(abs(resid))) %>% head(10)
```



## 

Can play around with the model to improve performance

\vspace{12pt}
\tiny
```{r}
m3 <- glm(Species ~ Elevation + I(Elevation^2) + Nearest + Scruz + 
            I(Scruz^2) + Adjacent + 
            Area + I(Area^2), family = "poisson", data = gala, x = TRUE)

foo <- data.frame(names = rownames(gala),
                  Species = gala$Species,
                  pred = as.numeric(predict(m3, type = "response")), 
                  resid = residuals(m3))
head(cbind(foo %>% arrange(desc(Species)) %>% pull(names),
           foo %>% arrange(desc(pred)) %>% pull(names)), 10)
```



##

This model offers improvements in residual magnitude and we see that problems are occurring for small species counts.

\vspace{12pt}
\tiny
```{r}
foo %>% arrange(desc(abs(resid))) %>% head(11)
```


```{r}
cbind(foo$pred, gala[, -8])[which(abs(foo$resid) > 4), ]

```



## 


\vspace{12pt}
\tiny
```{r}
plot(log(fitted(m3)),log((gala$Species-fitted(m3))^2), 
     xlab= expression(hat(mu)),ylab=expression((y-hat(mu))^2), pch = 19)
abline(0,1)
```



## 

\vspace{12pt}
\tiny
```{r}
theta <- as.numeric(m3$x %*% coef(m3))
par(mfrow = c(1,2))
plot(theta, residuals(m3), xlab = "theta", ylab = "Deviance residuals", pch = 19)
plot(theta, residuals(m3, "pearson"), pch = 19,  
     xlab = "theta", ylab = "Pearson residuals")
```