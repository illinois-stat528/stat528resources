% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  ignorenonframetext,
]{beamer}
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{part title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{part title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\newif\ifbibliography
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{graphicx}
\usepackage{bm}
\usepackage{array}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{tikz-cd}
\usepackage{url}
\definecolor{foreground}{RGB}{255,255,255}
\definecolor{background}{RGB}{34,28,54}
\definecolor{title}{RGB}{105,165,255}
\definecolor{gray}{RGB}{175,175,175}
\definecolor{lightgray}{RGB}{225,225,225}
\definecolor{subtitle}{RGB}{232,234,255}
\definecolor{hilight}{RGB}{112,224,255}
\definecolor{vhilight}{RGB}{255,111,207}
\setbeamertemplate{footline}[page number]
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={STAT 528 - Advanced Regression Analysis II},
  pdfauthor={Exponential family theory (part 2)},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}

\title{STAT 528 - Advanced Regression Analysis II}
\author{Exponential family theory (part 2)}
\date{}
\institute{Daniel J. Eck\\
Department of Statistics\\
University of Illinois}

\begin{document}
\frame{\titlepage}

\begin{frame}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Proj}{\textbf{P}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\rootn}{\sqrt{n}}
\newcommand{\p}{\mathbf{p}}
\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}

\newtheorem{cor}{Corollary}
\newtheorem{lem}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{prop}{Proposition}
\end{frame}

\begin{frame}{Last time}
\protect\hypertarget{last-time}{}
Let \(Y\) be a regular full exponential family in canonical form. Then
\(Y\) has log likelihood given by \[
  l(\theta) = \langle Y,\theta \rangle - c(\theta),
\] and \begin{align*} 
    \text{E}_\theta(Y) &= \nabla c(\theta),  \\
    \text{Var}_\theta(Y) &= \nabla^2 c(\theta).   
\end{align*}

Examples include: Binomial, Poisson, Normal, etc.
\end{frame}

\begin{frame}{Learning Objectives Today}
\protect\hypertarget{learning-objectives-today}{}
\begin{itemize}
\tightlist
\item
  mean value parameters
\item
  maximum likelihood estimation (MLE)
\item
  asymptotics of MLE
\item
  finite sample concentration of MLE
\end{itemize}
\end{frame}

\begin{frame}{Mean value parameters}
\protect\hypertarget{mean-value-parameters}{}
The mean of the canonical statistic \(\text{E}_\theta(Y)\) is also a
parameter.

It is given as a function \(g\) of the canonical parameter \(\theta\),
\begin{equation} \label{mvp}
  g(\theta) = \nabla c(\theta) = \text{E}_\theta(Y) = \mu.    
\end{equation}

We will refer to \(g:\theta\to\mu\) as the change-of-parameter map (or
change-of-parameter) from canonical parameter \(\theta\) to mean value
parameter \(\mu\).
\end{frame}

\begin{frame}{Example: Binomial}
\protect\hypertarget{example-binomial}{}
Recall that the log likelihood for the binomial distribution (after
dropping terms) \[
  y\log(p) + (n-y)\log(1-p)
\] can be written in canonical form with \[
  c(\theta) = n\log(1 + \exp(\theta)),
\] where \(\theta = \log(p/(1-p))\) and
\(p = \exp(\theta)/(1 + \exp(\theta))\). Now, \[
  \nabla c(\theta) = n \left(\frac{\exp(\theta)}{1 + \exp(\theta)}\right) = np,
\] and this is \(E_\theta(Y)\).
\end{frame}

\begin{frame}{Mean value parameters}
\protect\hypertarget{mean-value-parameters-1}{}
\begin{thm} \label{thm-mvp}
For a regular full exponential family, the change-of-parameter from canonical to mean value parameter is invertible if the model is identifiable. Moreover both the change-of-parameter and its inverse are infinitely differentiable.
\end{thm}

\vspace*{12pt}

See notes for proof.
\end{frame}

\begin{frame}{Mean value parameters}
\protect\hypertarget{mean-value-parameters-2}{}
Recall from last time that an exponential family is identifiable if and
only if the canonical statistic is NOT concentrated on a hyperplane
\begin{equation}\label{hyperplane}
  H = \{y : y^Tv = a\}  
\end{equation}

\vspace*{12pt}

\begin{thm}
An exponential family fails to be identifiable if and only if the canonical statistic is concentrated on a hyperplane. If that hyperplane is given by \eqref{hyperplane} and the family is full, then $\theta$ and $\theta+sv$ are in the full canonical parameter space and correspond to the same distribution for every canonical parameter value $\theta$ and every scalar $s$.     
\end{thm}
\end{frame}

\begin{frame}{Multivariate monotone}
\protect\hypertarget{multivariate-monotone}{}
The change-of-parameter mapping \(g:\theta\to\mu\) is multivariate
monotone, \begin{equation} \label{multiparm}
  (g(\theta_1) + g(\theta_2))'(\theta_1 - \theta_2) \geq 0,
\end{equation} where \(g(\theta_i) = \mu_i\), \(i = 1,2\).

If we rewrite \eqref{multiparm} using subscripts and consider
\(\theta_1\) and \(\theta_2\) that differ in only one coordinate, say
the \(k\)th, then we get \[
  (\mu_{1k} - \mu_{2k})(\theta_{1k} - \theta_{2k}) > 0,
\] which says
\emph{if we increase one component of the canonical parameter vector, leaving the other components fixed, then the corresponding component of the mean value parameter vector also increases, and the other components can go any which way}.
\end{frame}

\begin{frame}{MLE}
\protect\hypertarget{mle}{}
The derivative of the log likelihood is \[
  \nabla l(\theta) = y - \nabla c(\theta).
\] The second derivative is \[
  \nabla^2 l(\theta) = -\nabla^2 c(\theta).
\] Hence observed and expected Fisher information for the canonical
parameter vector \(\theta\) are the same \begin{equation} \label{FI}
    I(\theta) = \nabla^2 c(\theta).
\end{equation}
\end{frame}

\begin{frame}{}
\protect\hypertarget{section}{}
\textbf{Note}: Fisher information measures the expected curvature of the
log likelihood around the true parameter value.

If the likelihood is sharply curved around \(\theta\) -- the expected
information \(I(\theta)\) is large -- then a small change in \(\theta\)
can lead to a drastic decrease in the likelihood. Conversely, if
\(I(\theta)\) is small then small changes in \(\theta\) will not affect
the likelihood that much.

Similar heuristics are important when we cover data separation.
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-1}{}
When

\begin{itemize}
\tightlist
\item
  the model is identifiable
\item
  the canonical statistic vector \(Y\) is not concentrated on a
  hyperplane
\item
  the second derivative is negative definite everywhere.
\end{itemize}

Hence the log likelihood is strictly concave, hence the maximum
likelihood estimate is unique if it exists. Thus, \[
  y = \nabla c(\hat{\theta}),
\] and \[
  \hat{\theta} = g^{-1}(y).
\]
\end{frame}

\begin{frame}{Observed equals expected}
\protect\hypertarget{observed-equals-expected}{}
The MLE, if it exists, must be a point where the first derivative is
zero, that is, a \(\theta\) that satisfies \[
  y = \nabla c(\theta) = \text{E}_\theta(Y).
\] The MLE is the (unique if the model is identifiable) parameter value
that makes the observed value of the canonical statistic equal to its
expected value.

We call this the \textbf{observed equals expected} property of maximum
likelihood in exponential families.
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-2}{}
This property is even simpler to express in terms of the mean value
parameter. By invariance of maximum likelihood under
change-of-parameter, the MLE for \(\mu\) is \[
  \hat\mu = g(\hat\theta) = \nabla c(\hat\theta)
\] and the observed equals expected property is therefore
\begin{equation} \label{obsequalsexp}
  y = \hat\mu.  
\end{equation}
\end{frame}

\begin{frame}{Non-Existence of the MLE}
\protect\hypertarget{non-existence-of-the-mle}{}
We cannot prove the maximum likelihood estimate (for the canonical
parameter) exists.

Consider the binomial distribution. The MLE for the usual
parameterization is \(\hat p = y/n\).

The canonical parameter is \(\theta = \text{logit}(p)\).

But \(\hat \theta = \text{logit}(\hat p)\) does not exist when
\(\hat p = 0\) or \(\hat p = 1\), which is when we observe zero
successes or when we observe \(n\) successes in \(n\) trials.

We will revisit this topic when we discuss GLMs.
\end{frame}

\begin{frame}{IID Data}
\protect\hypertarget{iid-data}{}
Suppose \(y_1, \ldots, y_n\) are independent and identically distributed
(IID) from some distribution in an exponential family.

The log likelihood for sample size \(n\) is \begin{equation} \label{iid}
    l_n(\theta) = \sum_{i=1}^n\left[\langle y_i,\theta\rangle - c(\theta)\right] 
      = \langle\sum_{i=1}^n y_i, \theta\rangle - n c(\theta),
\end{equation} and we see that the above log likelihood is an
exponential family with:

\begin{itemize}
\tightlist
\item
  canonical statistic \(\sum_{i=1}^n y_i\),
\item
  cumulant function \(\theta \mapsto n c(\theta)\), and
\item
  canonical parameter \(\theta\) and full canonical parameter space
  \(\Theta\) the same as the originally given family from which every
  observation is a member.
\end{itemize}

Thus IID sampling gives us a new exponential family, but still an
exponential family.
\end{frame}

\begin{frame}{Asymptotics of MLE for regular full exponential family}
\protect\hypertarget{asymptotics-of-mle-for-regular-full-exponential-family}{}
Rewrite \eqref{iid} as \[
  l_n(\theta) = n\left[\langle \bar y_n, \theta\rangle - c(\theta)\right]
\] so that \[
  \nabla l_n(\theta) = n\left[\bar y_n - \nabla c(\theta)\right].
\] From which we see that for an identifiable regular full exponential
family where the MLE must be a point where the first derivative is zero,
we can write \begin{align*}
    \nabla l_n(\theta) = n\left[\bar y_n - \nabla c(\theta)\right] &= 0, \\
    \bar y_n - \nabla c(\hat\theta) &= 0.
\end{align*} And this yields \[
  \bar y_n = \nabla c(\hat\theta).
\]
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-3}{}
Recall the change-of-parameters mapping \(g:\theta \mapsto \mu\) is
invertible when the model is identifiable. We can write
\begin{equation} \label{MVPg}
  \hat\theta_n = g^{-1}(\bar y_n),  
\end{equation} when the MLE exists.

When the MLE does not exist, \(\bar y_n\) is not in the domain of
\(g^{-1}\).
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-4}{}
By the multivariate central limit theorem (CLT) \[
  \sqrt{n}\left(\bar y_n - \mu\right) \to N\left(0, I(\theta)\right)
\] and we know that \(g^{-1}\) is differentiable (Theorem \ref{thm-mvp}
in this slide deck) with the derivative given by \[
  \nabla g^{-1}(\mu) = \left[\nabla g(\theta)\right]^{-1},
\] where \[
  \mu = g(\theta) \;\;\; \text{and} \;\;\; \theta = g^{-1}(\mu).
\]
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-5}{}
So the usual asymptotics of maximum likelihood
\begin{equation} \label{asymptoticsMLE}
    \sqrt{n}\left(\hat\theta_n - \theta\right) \to N\left(0, I(\theta)^{-1}\right)
\end{equation} is just the multivariate delta method applied to the
multivariate CLT.
\end{frame}

\begin{frame}{Example: Bernoulli distribution}
\protect\hypertarget{example-bernoulli-distribution}{}
Let \(y_1,\ldots,y_n \overset{iid}{\sim}\) Bernoulli(\(p\)) where
\(0 < p < 1\).

Then \[
  l_n(\theta) = \langle\sum_{i=1}^n y_i,\theta\rangle - n c(\theta)
\] where \begin{align*}
  g(\theta) &= \frac{\exp(\theta)}{1 + \exp(\theta)} = p \\
  g^{-1}(p) &= \log\left(\frac{p}{1-p}\right) = \theta,
\end{align*} and \[
  \hat{p} = \bar{y}_n.
\]
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-6}{}
The expected Fisher information is \begin{align*}
  I(\theta) &= \nabla^2 c(\theta) \\
    &= \left(\frac{\exp(\theta)}{1 + \exp(\theta)}\right)\left(\frac{1}{1 + \exp(\theta)}\right) \\
    &= p(1-p).
\end{align*}

Therefore, \begin{align*}
 \sqrt{n}\left(\hat{\theta}_n - \theta\right) &\to N\left(0,  I(\theta)^{-1}\right) \\
   &= N\left(0,  \frac{1}{p(1-p)}\right).
\end{align*}
\end{frame}

\begin{frame}{Finite sample concentration of MLE (scalar case)}
\protect\hypertarget{finite-sample-concentration-of-mle-scalar-case}{}
\begin{defn}
A random variable $Y$ with mean $\mu = \text{E}(Y)$ is \emph{sub-exponential} if there exist non-negative numbers $(\lambda,b)$ such that 
$$
  E\left(e^{\phi(Y - \mu)}\right) \leq e^{\lambda^2\phi^2/2} 
    \qquad \text{for all} \; |\phi| < 1/b.
$$  
\end{defn}

\vspace{12pt}

Let \(Y\) be a scalar canonical statistic regular full exponential
family with canonical parameter \(\theta\), then \(Y\) is
sub-exponential.
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-7}{}
We can show the MLE of \(\theta\) exhibits sub-exponential concentration
following the logic that \((\hat\theta - \theta)\) has the same tail
bounds as a sub-exponential random variable.

We can use these results to obtain the rate of convergence \[
  \mathbb{P}\left( (\hat\theta - \theta) \geq \frac{\log(n)}{n}\right) 
    = O\left(n^{-\frac{A}{2b}}\right),
\] where \(t = \log(n)/n\) for some numbers \(A\) and \(b\).
\end{frame}

\end{document}
